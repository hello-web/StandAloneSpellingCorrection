{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasetUtil import *\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_billion_path = \"1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00000-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00001-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00002-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00003-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00004-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00005-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00006-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00007-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00008-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00009-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00010-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00011-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00012-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00013-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00014-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00015-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00016-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00017-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00018-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00019-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00020-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00021-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00022-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00023-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00024-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00025-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00026-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00027-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00028-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00029-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00030-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00031-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00032-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00033-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00034-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00035-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00036-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00037-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00038-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00039-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00040-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00041-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00042-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00043-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00044-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00045-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00046-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00047-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00048-of-00050\n",
      "1-billion-word-language-modeling-benchmark-r13output/heldout-monolingual.tokenized.shuffled/news.en.heldout-00049-of-00050\n"
     ]
    }
   ],
   "source": [
    "one_billion_sentence_tokens = readOneBillion(one_billion_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306688/306688 [02:03<00:00, 2491.61it/s]\n",
      "  4%|▍         | 12602/306688 [00:00<00:02, 126019.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed to ERNIE tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306688/306688 [00:02<00:00, 104597.91it/s]\n"
     ]
    }
   ],
   "source": [
    "one_billion_sentence_tokens = adaptERNIEtokenization(one_billion_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306688/306688 [00:04<00:00, 74530.35it/s]\n"
     ]
    }
   ],
   "source": [
    "mergedCounts = countCorpus(one_billion_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#char_path = \"charVocab.txt\"\n",
    "#word_path = \"wordVocab.txt\"\n",
    "#charList, wordList = readVocab(char_path, word_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequentWordCount = mergedCounts[:30000]\n",
    "wordList = [word for word, count in frequentWordCount]\n",
    "charList = charVocab(mergedCounts)\n",
    "with open(\"wordVocab.txt\",\"w\") as f:\n",
    "    for word in wordList:\n",
    "        f.write(word+\"\\n\")\n",
    "    f.write(\"<unk>\"+\"\\n\")\n",
    "    f.write(\"<oov>\"+\"\\n\")\n",
    "with open(\"charVocab.txt\",\"w\") as f:\n",
    "    for char in charList:\n",
    "        f.write(char+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_one_billion_sentence_tokens = filterOutlierSentence(one_billion_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306544"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_one_billion_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens, _ = excludeSentenceWithUnknownWords(wordList, oov_one_billion_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191005"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 5000\n",
    "test_sentences = filtered_tokens[:N_test]\n",
    "dev_sentences = filtered_tokens[N_test:2*N_test]\n",
    "train_sentences = filtered_tokens[2*N_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181005\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences))\n",
    "print(len(dev_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train1B.txt\",\"w\") as f:\n",
    "    for sentence in train_sentences:\n",
    "        f.write(\" \".join(sentence)+\"\\n\")\n",
    "with open(\"dev1B.txt\",\"w\") as f:\n",
    "    for sentence in dev_sentences:\n",
    "        f.write(\" \".join(sentence)+\"\\n\")\n",
    "with open(\"test1B.txt\",\"w\") as f:\n",
    "    for sentence in test_sentences:\n",
    "        f.write(\" \".join(sentence)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "berkbeck = read_berkbeck(\"missp.dat.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_replacement = readYonatan(\"en.natural.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedWordReplacement = mergeWordReplacement(berkbeck, natural_replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainWordReplacement = sampleReplacement(mergedWordReplacement, 0.8)\n",
    "#with open(\"train_replacement.json\",\"w\") as f:\n",
    "#    json.dump(trainWordReplacement, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_replacement.json\") as f:\n",
    "    trainWordReplacement = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181005it [00:05, 32747.55it/s]\n"
     ]
    }
   ],
   "source": [
    "replaced_one_billion_sentence_tokens, gold_labels = wordReplacement(trainWordReplacement, train_sentences, word_vocab = None, sigma=0.2)\n",
    "saveNoisyInput(\"train.txt\", replaced_one_billion_sentence_tokens)\n",
    "saveNoisyInputLabel(\"train_label.jsonl\", gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 35036.33it/s]\n"
     ]
    }
   ],
   "source": [
    "replaced_one_billion_sentence_tokens, gold_labels = wordReplacement(trainWordReplacement, dev_sentences, word_vocab = None, sigma=0.2)\n",
    "saveNoisyInput(\"dev.txt\", replaced_one_billion_sentence_tokens)\n",
    "saveNoisyInputLabel(\"dev_label.jsonl\", gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 37385.99it/s]\n"
     ]
    }
   ],
   "source": [
    "replaced_one_billion_sentence_tokens, gold_labels = wordReplacement(mergedWordReplacement, test_sentences, word_vocab = None, sigma=0.2)\n",
    "saveNoisyInput(\"test.txt\", replaced_one_billion_sentence_tokens)\n",
    "saveNoisyInputLabel(\"test_label.jsonl\", gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181005it [00:13, 13808.35it/s]\n"
     ]
    }
   ],
   "source": [
    "replaced_one_billion_sentence_tokens, gold_labels = charReplacement(train_sentences, word_vocab = None, sigma=0.2)\n",
    "saveNoisyInput(\"char_train.txt\", replaced_one_billion_sentence_tokens)\n",
    "saveNoisyInputLabel(\"char_train_label.jsonl\", gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
